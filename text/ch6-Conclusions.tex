\chapter{\label{ch6-Conclusions} Conclusions}
\minitoc
\section{Outlook}
Throughout this thesis, we have explored the complex task of performing deep-learning based event classification for stereoscopic IACT arrays, as well as exploring the background present in IACT images due to the night sky. The results presented include a number of interesting discoveries which we will now summarise.

In Chapter \ref{ch:3-TimingInfo}, we have seen that high-precision timing information can offer a clear advantage over conventional charge images when dealing with deep-learning-based $\gamma$-hadron separation for SSTCAM on simulations. However, the full sensitivity analysis for this method will not be possible until a full CTA analysis chain is in place. Given the potential significant performance boost over integrated charge images, it is possible that the timing methods we demonstrate might perform better with real data compared to charge data analysis, but we will need to wait until there is an SST prototype on the Paranal site in order to verify this. In Chapter \ref{ch:4-VERITASRealData}, we have seen the complexity involved in applying deep-learning-based event classification methods to IACT charge data from VERITAS. Our most notable discovery is that it is indeed possible to perform real detections in IACT data of real sources, without resorting to image cleaning or strenuous cuts. In particular, we have found that Bayesian optimisation can offer significant improvements in classifier performance on both simulated and real observations, although it is currently highly computationally intensive. Despite the fact that it appears we hit the limit of what is currently possible with the training data and optimiser we used, future advances in model optimisation might help make deep learning analyses competitive with the current state-of-the-art techniques. Part of the difficulty of applying deep learning techniques to VERITAS data was the rigidity of the analysis chain. The current development plan for \textit{ctapipe} is to provide a plugin framework for alternative event reconstruction and classification methods, such a plugin framework might be able to overcome the issues with using real observations as test data that we encountered. Finally in Chapter \ref{ch:5-CHECNSB}, we have demonstrated one area of improvement in NSB simulation (over \textit{sim\_telarray}) that might aid deep learning classifiers in the future, and provided NSB rate estimates useful for the design of SSTCAM. This work appears to demonstrate that SSTCAM will meet most of the associated NSB requirements, even under some extreme observing conditions. That said, some small fraction of pixels may still need to be disabled and removed from the Cherenkov analysis if they fall under the light from a bright star whilst observing during bright moonlight.

From the results in this thesis, it appears clear that there is still much work to be done in regards to determining the ideal training datasets (in terms of simulation setup, image cleaning and event selection cuts) to be used for deep-learning-based event classification, let alone full event reconstruction (where this is performed simultaneously with directional and energy reconstruction). Comparing our results between SSTCAM and VERITAS suggests that among the three CTA instrument classes (which will share a similar analysis structure) there may need to be differing analyses used per-class due to the potential effects of differing background spectra, different timing accuracies, and different plate scales/camera architectures. There is a reasonable argument that we should approach each of the three reconstruction tasks separately in order to definitively associate improvements in sensitivity with a particular analysis technique for a particular analysis task. Nonetheless, the potential for a stereoscopic, full-image-based, fast analysis for IACT arrays using deep learning remains a tantalising prospect that will without doubt remain a significant area of research for several years to come. Given the rapid advance of deep learning research, it is entirely feasible that analysis methods more robust to instrument noise and NSB than CNNs may become available in the near future, and our experience with CNN-type methods will provide invaluable lessons about how to approach such tasks. But we must be careful not to mistake deep learning model complexity for physical understanding of our instruments and the associated particle astrophysics in the meantime.

\section{Future Prospects}
To conclude, we will explore the potential avenues of further investigation that could be performed following the work in this thesis.

\subsection{Bayesian Neural Networks and Probability Maps}

As mentioned in Chapters \ref{ch:2-CNNs} and \ref{ch:4-VERITASRealData}, the current classification scores for individual events from CNN-type methods are not necessarily meaningful on their own. Neural network classifiers are subject to both epistemic uncertainty (uncertainty associated with data quality) and aleatoric uncertainty (associated with network models being imperfect). Research utilising Bayesian deep learning is a growing field in astronomy, but is more limited in astroparticle physics, with only preliminary conference paper on the topic existing \cite{bayesianwcd}. Iterative dropout methods are one such means of estimating deep-learning classifier uncertainty; they approximate an ensemble of different networks by leaving neurons free to drop out during testing (normally this is only done during training and the dropped-out neurons are frozen for testing). Some work has been performed on the subject of implementing `Bayesian' iterative dropout methods for ConvLSTM architectures \cite{bayesconv}, but this work is at an early stage. Stable, open source implementations for this are not available yet. Our own experiments to implement these for ConvLSTM architectures for CHEC-S data produced classifier results not better than random-guessing, even on simulated test data.

One potential option that has seen use in certain Zooniverse projects \cite{mike}, and would be significantly simpler to implement, is to simply train multiple classifiers with different starting seeds and then average the output scores for given events. With greater available GPU power this might become feasible in the near future for CTA.

Another option for directional and energy reconstruction has recently been proposed by Borquez et al. [in prep.]. The suggested technique is to perform upsampling on the output of a CNN with a dense backend to produce one or two dimensional probability maps for the task in question. These can be used to generate an stereoscopic uncertainty map for the position and energy of an event, however such techniques appear to currently be highly computationally costly and difficult to perform at scales necessary for CTA.

\subsection{Data Augmentation}
Data augmentation has been proposed \cite{aug} as a method of forcing neural networks to generalise to wider ranges of experimental conditions. However, in the specific case of stereoscopic IACT analysis, we posit that it's unlikely to help. One common means of performing such augmentation to improve robustness to noise is data augmentation with Gaussian noise \cite{bayesdeblend}, but as we have seen in Chapter \ref{ch:5-CHECNSB} the NSB observed by IACT cameras is not Gaussian. One potential option would be to augment advanced NSB analysis, like that in Chapter \ref{ch:5-CHECNSB}, into training data generated from \textit{CORSIKA/sim\_telarray}; given an NSB rate in Hz it is possible to compute the number of photoelectrons that would be detected in a $\mathrm{\sim 96\,ns}$ observing window. But performing such an augmentation to give an accurate pixel-wise prediction (to the sensitivity level of a CNN-type method) for NSB rates would likely be highly challenging. This would involve significant computational cost, a result of the sensitivity of SiPMs to dim stars (requiring many stars to be modelled) and the potentially rapid shifts in NSB over time. Similarly, augmenting the data by rotating images (which is a way of creating robustness to rotation \cite{Keras}) may not work in practice given the complex geometry of an IACT array.

\subsection{Advanced Model Selection Methods}

It should be noted that the Bayesian Optimisation presented in Chapter \ref{ch:4-VERITASRealData} is a comparatively simple method of model optimisation. In industry, more advanced methods of model selection are becoming available, such as evolutionary algorithms \cite{evodeep} (which simulate genetic mutation) and neural architecture searches \cite{neural}. These methods are slowly becoming more widely used. But, in general, such methods where the entire model architecture can be optimised (and not just hyperparameters) are extremely computationally intensive, even more so than the work in Chapter \ref{ch:4-VERITASRealData}. At this level of optimisation complexity, end users such as the CTA collaboration cannot reasonably acquire sufficient compute power to perform these analyses. This might change in the coming years.

\subsection{GAN-Based Simulations and Domain Adaptation}
There is a wide body of computer science literature on using different variants of GAN (such as SimGAN \cite{simgan} and cyclegan \cite{cyclegan}) as a method of handling domain shifts (such as those between simulated and real observations). These typically rely on enforcing cyclic consistency between performance on different datasets. However, our own investigations of applying such methods to CHEC-S data, whereby we attempted to evaluate performance on a test dataset that had minor discrepancies (such as zeroed pixels) applied to it, were unsuccessful (it was better not to use the GANs whatsoever). This might be a future source of potential methods to handle the real observations problem, but we believe it's unlikely. We found the difficulty of adapting GANs to different datasets, the complexity of training them, the difficulty interpreting their behaviour, as well as the additional computational cost, make these methods likely unfeasible. Another issue is the lack of a consistent framework to apply such methods in practice. Whilst there have been attempts at creating such a framework (the best of which is \textit{Keras-GAN} \cite{kerasgan}), there are simply too many alternative GAN models to explore every option in every use case. 

Alternatively, GANs can be used as a fast pseudo-simulation method. Whilst this is unlikely to be useful for replacing \textit{CORSIKA}/\textit{sim\_telarray} for simulating EAS (our attempts at this suffered from so called `mode-collapse', whereby similar images are repeatedly generated), one potentially useful use of this (proportionately fast) technique would be as a simulator of NSB (using training data similar to that in Chapter \ref{ch:5-CHECNSB}). This is as GANs appear reasonably reliable at pseudo-simulation of noise (see for example results on CMB data in \cite{darshgan}); this might prove a faster means of NSB data augmentation than directly using \textit{nsb} (as suggested earlier in this chapter). 

\subsection{Graph Networks}
Given the results in Chapter \ref{ch:4-VERITASRealData}, it seems likely that current CNN-based methods of event classification reliant on integrated charge data (and not utilising timing information as in Chapter \ref{ch:3-TimingInfo}) will in practice be limited by NSB and instrumental noise unless tailcut cleaning is applied.

In work outside this thesis, we recently proposed using new timing based methods would be to use alternative deep learning classifiers to CNNs with charge data. One such method is graph-based Chebyshev networks, where the image data is treated as a 2D connected graph and not a 2D Euclidean array. One advantage of this is that it could potentially skip the image mapping step needed for CNN-type analyses, though recent results from A. Jacobson \cite{adithesis} suggest that these methods are currently computationally prohibitive, being too VRAM hungry and slow to be of practical use. There is also no current means of performing stereoscopic analysis with such techniques. This might change in upcoming years, with research into graph-based methods being currently extremely active, even to the extent that it leads to advances in GPU technology  \cite{graphcore}.

\subsection{Transfer Learning}
One potential solution to the hyperparameter problem is the option to take pre-trained and optimised classifiers such as ResNet50 \cite{resnet50} (which is trained on the standard computer science dataset ImageNet \cite{ImageNet} consisting of images of everyday objects), and then slightly retrain the final weights in such a network on IACT data. This is similar to the approach used in a recent paper by Miener et al. \cite{tjarkicrc}. This is very exciting work, and represents the first event classification results at realistic CTA scales. But techniques to adapt transfer learning methods to stereoscopic IACT analysis are in their relative infancy. Currently they don't appear to offer large improvements in effective area relative to BDT analysis as a result (on simulated data). We speculate that this is possibly because the weights in the network are currently frozen prior to the sequence analysis stage (this issue would be solvable with increased GPU power). That said, there are new deep-transfer-learning techniques (such as \textit{Perceiver} models \cite{perceiver}) being developed by computer scientists for multi-modal data that may be suitable for IACT analysis that could be used `out the box'. Development of such multi-model methods that support flexibility in the type and shapes of the input data will likely be highly useful in future work in the physical sciences.

\subsection{Larger Arrays}
Much of the work presented in Chapters \ref{ch:3-TimingInfo} and \ref{ch:4-VERITASRealData} was GPU resource limited, meaning that using simulated arrays truly representative of the SST component of CTA were not possible. The Miener et al. \cite{tjarkicrc} paper has shown the first CNN-based results from an array of 40 SSTs using the \textit{CTLearn} framework, but this required significant time on the Wilkes-2 GPU cluster at Cambridge to perform. Optimising such networks beyond what has already been performed would be extremely difficult and computationally intensive, but the power of future GPU clusters may make this tractable. But this still does not solve the problem with simultaneously utilising multiple telescope classes with Shilon-like deep learning event classification methods.

In the long term, even if CNN-type methods can only be demonstrated to offer increased background rejection power for isolated point sources and large arrays, this could aid in providing a sufficient signal-to-noise ratio to perform spectral analysis on short timescale observations. This might be useful for the observation of short GRBs from the ground.

\subsection{FPGA Analysis and Photon Stream Analysis}

FACT have recently presented results showing the potential use of Field Programmable Gate Arrays to enhance the speed of deep-learning-based event classification at inference time \cite{factfpga}. Whilst novel and interesting, the fact using FPGAs innately limits the analysis to monoscopic reconstruction limits the current potential benefits for sensitivity (since model complexity constraints are enforced when performing such analyses on such simple hardware). However, such FPGA-based methods may be an interesting avenue of investigation in the long term, such as in their use for muon tagging. This mono-analysis FACT paper also used timing information in a similar way to our CHEC methods to aid in event classification (albeit in a more computationally costly way than use in Chapter \ref{ch:3-TimingInfo}).

One promising avenue of investigation for SSTCAM is to implement the `photon stream' analysis used by FACT \cite{factphotonstream}; this uses an iterative cross-correlation procedure to extract single photons from their SiPM waveforms, before applying DBSCAN clustering analysis to extract the Cherenkov light. This could lead to improved analysis methods for SSTCAM and might make transfer of such data from Cerro Paranal feasible given the resulting compression factor. This transfer probably wouldn't feasible for full waveforms as the data rate would hit 1Gb/s \cite{trigrate} if they were to be transferred off-site.

\subsection{Upgrading Conventional Analysis}
One largely unexplored area of IACT analysis is taking optimisation methods designed for deep learning analyses and applying them to conventional Hillas-parameter-based analysis. In particular, Bayesian optimisation (and more generally the field of AutoML, particularly packages such as \textit{auto-sklearn} \cite{autosklearn}) could be used to optimise Hillas parameter cuts and BDT/RF hyperparameters to achieve improved performance. Additionally, recent developments with the RAPIDS \cite{rapids} library mean GPUs can be used to accelerate BDT and RF training, a useful tool given the data scales of CTA \cite{trigrate}. This is an area of research that should be prioritised, as it could lead to immediate, computationally feasible performance improvements for both the current generation of IACTs and CTA.