\chapter{\label{ch6-Conclusions} Conclusion}
\minitoc
\section{Outlook}

In chapter \ref{ch:3-TimingInfo}, we have seen that high precision timing information can offer a clear advantage over conventional charge images when dealing with $\gamma$-hadron separation for SSTCAM. However the full sensitivity analysis for this method will not be possible until a full CTA analysis chain is in place. In chapter \ref{ch:4-VERITASRealData}, we have seen the complexity in applying existing methods to IACT data from VERITAS, however it is notable that it is possible to perform real detections in IACT data of real sources, without resorting to image cleaning. In particular, we have found that Bayesian optimisation can offer significant improvements in classifier performance on both simulated and real data, although it is currently highly computationally intensive. Finally in chapter \ref{ch:5-CHECNSB}, we have demonstrated one area of improvement in NSB simulation (over sim\_telarray) that might aid deep learning classifiers in the future.

\section{Future Prospects}

\subsection{Bayesian Neural Networks and Probability Maps}

As mentioned in Chapters \ref{ch:2-CNNs} and \ref{ch:4-VERITASRealData}, the classification scores for individual events from CNN-type methods are not necessarily meaningful on their own.

Some work has been performed in the computer science literature on the subject of implementing such 'Bayesian' iterative dropout methods for ConvLSTM architectures \cite{bayesconv},  but this work is extremely prototypical and stable, open source implementations are not available yet. These approximate an ensemble of different networks by leaving neurons free to dropout during testing (normally this is only done during training and the dropped-out neurons are frozen for testing). However, such iterative dropout methods are difficult to implement for more complex CNN-RNN/ConvLSTM type architectures, and my own experiments to implement these have resulted in $\sim$ random guessing performance on even simulated test data.

One potential option that has seen use in certain Zooniverse projects, and would be significantly simpler to implement, is to simply train multiple classifiers with different starting seeds and then average the output scores for given events. With greater available GPU power this might become feasible in the near future for CTA.

Another option for directional and energy reconstruction that has recently been proposed by a Chilean group is to perform upsampling on the output of a CNN with a dense backend to produce one or two dimensional probability maps for the task in question.

\subsection{Data Augmentation}
Data augmentation has been proposed CITE as a method of forcing neural networks to generalise to wider ranges of experimental conditions. However, in the specific case of stereoscopic IACT analysis, I think it's unlikely to help. For augmentation with gaussian noise, one option for handling varying noise conditions, we have seen in chapter \ref{ch:5-CHECNSB} that the NSB observed by the cameras is nowhere near gaussian. Similarly, for augmenting the data by rotating images, which is a way of creating robustness to rotation, this may not work in practice given the complex geometry of an IACT array.
\subsection{Advanced Model Selection Methods}

It should be noted that the Bayesian Optimisation presented in Chapter \ref{ch:4-VERITASRealData} is a comparatively simple method of model optimisation. In industry, more advanced methods of model selection are becoming available, such as evolutionary algorithms \cite{evodeep} and neural architecture searches \cite{neural}. However, in general such methods where the entire model architecture can be optimised (and not just hyperparameters) are extremely computationally intensive, even more so than the work in Chapter \ref{ch:4-VERITASRealData}, to the point at which end users such as the CTA collaboration cannot reasonably acquire sufficient compute power to perform them. This might change in the coming years.

\subsection{Upgrading Conventional Analysis}
One largely unexplored area in terms of prospects for CTA, is taking optimisation methods designed for deep learning analyses and applying them to conventional Hillas-based analysis. In particular, Bayesian optimisation (and more generally the field of AutoML, particularly packages such as \textit{auto-sklearn}\cite{autosklearn}) could be used to optimise Hillas parameter cuts and BDT/RF hyperparameters, and recent developments using the RAPIDS \cite{rapids} library mean GPUs can be used to accelerate BDT and RF training, useful given the data scales of CTA.


\subsection{GAN Based Simulations and Domain Adaptation}
There is a wide body of computer science literature on using different variants of GAN, such as SimGAN \cite{simgan} and cyclegan \cite{cyclegan}, as a method of handling domain shifts (such as that between simulated and real data). These rely on enforcing cyclic consistency between performance on different datasets. However, my own investigations of applying such methods to CHEC-S data whereby I attempted to evaluate performance on a test dataset that had minor discrepancies (such as zeroed pixels) applied to it were an abject failure (it was better not to use the GANs whatsoever. This might be a future source of potential methods to handle the real data problem, but I personally think that it's unlikely. The difficulty of adapting GANs to different datasets, the complexity of training them, and the difficulty interpreting them, as well as the additional computational cost, make this methods likely unfeasible. Another issue is the lack of a consistent framework to apply such methods in practice. Whilst there have been attempts at creating such a framework (the best of which is \textit{Keras-GAN} \cite{kerasgan}), there are simply too many alternative GAN models to explore every option in every use case. 

Alternatively, GANs can be used as a fast pseudo-simulation method. Whilst this is unlikely to be useful for replacing CORSIKA/sim\_telarray (our attempts at this also failed, due to so called mode-collapse, whereby similar images are repeatedly generated), one potentially useful use of this (proportionately fast) technique would be as a simulator of NSB, as GANs appear reasonably reliable at pseudo-simulation of noise (see for example results on CMB data in \cite{darshgan}). 

\subsection{Graph Networks}
Given the results in Chapter \ref{ch:4-VERITASRealData}, it seems unlikely that CNN-based methods of event classification based on charge data (and not utilising timing information as in Chapter \ref{ch:3-TimingInfo}) will be prohibitively limited by NSB and instrumental noise unless tailcut cleaning is applied.

An alternative to using new timing based methods would be to use alternative deep learning classifiers to CNNs. One such method is graph-based Chebyshev networks, where the image data is treated as a 2D connected graph and not a 2D Euclidean array. One advantage of this is that it could potentially skip the image mapping step needed for CNN-type analyses, though recent results from \cite{adithesis} suggest that these methods are currently computationally prohibitive, being too VRAM hungry and slow to be of practical use. Additionally, there is no current means of performing stereoscopic analysis with such techniques. This might change in upcoming years, with research into graph-based methods being currently extremely active, and GPU advances can follow research trends.

\subsection{FPGA Analysis and Photon Stream Analysis}

FACT have recently presented results showing the potential use of Field Programmable Gate Arrays to enhance the speed of deep learning based event classification at inference time. Whilst novel and interesting, the fact that this innately limits the analysis to monoscopic reconstruction limits the current potential benefits for sensitivity (since model complexity constraints are enforced when performing such analyses on such simple hardware). However, this may be an interesting avenue of investigation for these analyses methods in the long term. The use of such FPGA-based methods for muon tagging is an interesting potential avenue of investigation. This FACT paper also used timing information in a similar way to our CHEC methods to aid in event classification (albeit in a more computationally costly way than use in Chapter \ref{ch:3-TimingInfo}).

CHEC photon stream stuff when it happens
\subsection{Transfer Learning}
One potential solution to the hyperparameter problem for CNN-type methods is the option to take pre-trained and optimised classifiers such as ResNet50 \cite{resnet50} (which is trained on the standard computer science dataset ImageNet) and then slightly retrain the final weights in such a network on IACT data. This is part of the approach used in \cite{tjarkicrc}. However the methods to adapt these methods to stereoscopic analysis are in their infancy, and currently don't appear to offer substantial improvements in sensitivity as a result. Similarly such methods are highly VRAM intensive and as such the overall benefit of using these methods as opposed to training/optimising a classifier from scratch are unclear.


\subsection{Larger Arrays}
Much of the work presented in Chapters \ref{ch:3-TimingInfo} and \ref{ch:4-VERITASRealData} was GPU resource limited, meaning that using simulated arrays truly representative of the SST component of CTA were not possible. Extremely recent, preliminary work by Meiner et al. \cite{tjarkicrc} has shown first CNN-based results from an array of 40 SSTs using the \textit{CTLearn} framework, but this required significant time on the Wilkes-2 GPU cluster at Cambridge to perform. Optimising such networks beyond what has already been performed would be extremely difficult and computationally intensive, but the power of future GPU clusters may make this tractable. This still does not solve the multiple class telescope problem, however.

In the long term, even if CNN-type methods can be demonstrated to offer increased background rejection power for isolated point sources, this could aid in providing sufficient signal to noise to perform spectral analysis, which might be nessecary for the study of short GRBs from the ground.