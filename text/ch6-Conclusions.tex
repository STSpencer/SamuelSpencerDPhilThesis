\chapter{\label{ch6-Conclusions} Conclusion}
\minitoc
\section{Outlook}
Throughout this thesis, we have explored the complexity in applying deep learning as an event classification method for stereoscopic IACT arrays, as well as exploring the complex background present in IACT images due to the night sky. The results present demonstrate a number of interesting discoveries which we will now summarise.

In Chapter \ref{ch:3-TimingInfo}, we have seen that high precision timing information can offer a clear advantage over conventional charge images when dealing with $\gamma$-hadron separation for SSTCAM using simulations. However the full sensitivity analysis for this method will not be possible until a full CTA analysis chain is in place. The current development plan for \textit{ctapipe} is to provide a plugin framework for alternative event reconstruction and classification methods, such a plugin framework might be able to overcome the issues with testing real observations we encountered in Chapter \ref{ch:4-VERITASRealData}. Given the potential significant performance boost over charge images, it is possible that the timing methods we demonstrate might perform better with real data compared to charge data analysis, but we will need to wait until there is a stable SST prototype on site in order to verify this. In Chapter \ref{ch:4-VERITASRealData}, we have seen the complexity in applying deep-learning-based event classification methods to IACT data from VERITAS, most notably that it is indeed possible to perform real detections in IACT data of real sources, without resorting to image cleaning or strenuous cuts. In particular, we have found that Bayesian optimisation can offer significant improvements in classifier performance on both simulated and real observations, although it is currently highly computationally intensive. Though it appears we hit the limit of what is currently possible with the training data and optimiser we used, future advances in model optimisation might help make deep learning analyses competitive with the current state-of-the-art techniques. Finally in Chapter \ref{ch:5-CHECNSB}, we have demonstrated one area of improvement in NSB simulation (over sim\_telarray) that might aid deep learning classifiers in the future, and provided NSB rate estimates useful for the design of SSTCAM. This work appears to demonstrate that SSTCAM will meet most of the associated NSB requirements, even under some extreme observing conditions. That said, some small fraction of pixels may still need to be disabled and removed from the Cherenkov analysis if they fall under the light from a bright star under bright moonlight.

from the results in this thesis, is appears clear that there is still much work to be done in regards to determining the ideal training datasets (in terms of simulation setup, image cleaning and event selection cuts) to be used for even deep-learning-based event classification, let alone full event reconstruction (where this is performed simultaneously with directional and energy reconstruction). Comparing our results between SSTCAM and VERITAS suggests that even among the three CTA instrument classes (which will share a similar analysis structure) there may need to be differing analyses used per-class due to the potential effects of differing background spectra, different timing accuracies, and different plate scales/camera architectures. There is a reasonable argument that we should approach each of the three reconstruction tasks separately in order to definitively associate improvements in sensitivity with a particular analysis technique for a particular analysis task. Nonetheless, the potential for a stereoscopic, full-image-based, fast analysis for IACT arrays from deep learning remains a tantalising prospect that will no doubt remain a significant area of research for several years to come. Given the rapid advance of deep learning research, it is entirely feasible that similar analysis methods more robust to noise and NSB than CNNs may become available in the near future, and our experience with CNN-type methods will provide invaluable lessons about how to approach such tasks. But we must be careful not to mistake deep learning model complexity for physical understanding of our instruments and the associated particle astrophysics in the meantime.

\section{Future Prospects}
To conclude, we will explore the potential avenues of further investigation that could be performed following the work in this thesis.

\subsection{Bayesian Neural Networks and Probability Maps}

As mentioned in Chapters \ref{ch:2-CNNs} and \ref{ch:4-VERITASRealData}, the current classification scores for individual events from CNN-type methods are not necessarily meaningful on their own. Neural network classifiers are subject to both epistemic uncertainty (uncertainty associated with data quality) and aleatoric uncertainty associated with network models being imperfect. Bayesian deep learning research is a growing field in astronomy, but research is more limited in the astroparticle physics, with only preliminary conference papers on the topic existing \cite{bayesianwcd}. Some work has been performed in the computer science literature on the subject of implementing `Bayesian' iterative dropout methods (a means of estimating these uncertainties) for ConvLSTM architectures \cite{bayesconv},  but this work is extremely prototypical and stable, open source implementations are not available yet. These approximate an ensemble of different networks by leaving neurons free to dropout during testing (normally this is only done during training and the dropped-out neurons are frozen for testing). Our experiments to implement these have resulted in $\sim$random-guessing performance on even simulated test data.

One potential option that has seen use in certain Zooniverse projects \cite{mike}, and would be significantly simpler to implement, is to simply train multiple classifiers with different starting seeds and then average the output scores for given events. With greater available GPU power this might become feasible in the near future for CTA.

Another option for directional and energy reconstruction that has recently been proposed by a Chilean group [Borquez et al., in prep] is to perform upsampling on the output of a CNN with a dense backend to produce one or two dimensional probability maps for the task in question. These can be used to generate an stereoscopic uncertainty map for the position and energy of an event, however such techniques appear to currently be highly computationally costly and difficult to perform at scale.

\subsection{Data Augmentation}
Data augmentation has been proposed \cite{aug} as a method of forcing neural networks to generalise to wider ranges of experimental conditions. However, in the specific case of stereoscopic IACT analysis, I think it's unlikely to help. For augmentation with Gaussian noise, one option for handling varying noise conditions, we have seen in Chapter \ref{ch:5-CHECNSB} that the NSB observed by the cameras is nowhere near Gaussian. Similarly, for augmenting the data by rotating images, which is a way of creating robustness to rotation, this may not work in practice given the complex geometry of an IACT array. One potential option would be to augment advanced NSB analysis, like that in Chapter \ref{ch:5-CHECNSB} into training data generated from \textit{CORSIKA/sim\_telarray}; given an NSB rate in Hz it is possible to compute the number of photoelectrons that would be detected in a $\sim$96ns observing window. But performing such an augmentation to give an accurate pixel-wise prediction (to the sensitivity level of a CNN-type method) for NSB rates would likely be highly challenging.

\subsection{Advanced Model Selection Methods}

It should be noted that the Bayesian Optimisation presented in Chapter \ref{ch:4-VERITASRealData} is a comparatively simple method of model optimisation. In industry, more advanced methods of model selection are becoming available, such as evolutionary algorithms \cite{evodeep} and neural architecture searches \cite{neural}. However, in general such methods where the entire model architecture can be optimised (and not just hyperparameters) are extremely computationally intensive, even more so than the work in Chapter \ref{ch:4-VERITASRealData}. At this level of optimisation complexity, end users such as the CTA collaboration cannot reasonably acquire sufficient compute power to perform these analyses. This might change in the coming years.

\subsection{Upgrading Conventional Analysis}
One largely unexplored area in terms of prospects for CTA, is taking optimisation methods designed for deep learning analyses and applying them to conventional Hillas-based analysis. In particular, Bayesian optimisation (and more generally the field of AutoML, particularly packages such as \textit{auto-sklearn} \cite{autosklearn}) could be used to optimise Hillas parameter cuts and BDT/RF hyperparameters, and recent developments using the RAPIDS \cite{rapids} library mean GPUs can be used to accelerate BDT and RF training, which is useful given the data scales of CTA.


\subsection{GAN-Based Simulations and Domain Adaptation}
There is a wide body of computer science literature on using different variants of GAN, such as SimGAN \cite{simgan} and cyclegan \cite{cyclegan}, as a method of handling domain shifts (such as that between simulated and real observations). These rely on enforcing cyclic consistency between performance on different datasets. However, my own investigations of applying such methods to CHEC-S data whereby I attempted to evaluate performance on a test dataset that had minor discrepancies (such as zeroed pixels) applied to it were an abject failure (it was better not to use the GANs whatsoever). This might be a future source of potential methods to handle the real observations problem, but I personally think that it's unlikely. The difficulty of adapting GANs to different datasets, the complexity of training them, the difficulty interpreting their behaviour, as well as the additional computational cost, make this methods likely unfeasible. Another issue is the lack of a consistent framework to apply such methods in practice. Whilst there have been attempts at creating such a framework (the best of which is \textit{Keras-GAN} \cite{kerasgan}), there are simply too many alternative GAN models to explore every option in every use case. 

Alternatively, GANs can be used as a fast pseudo-simulation method. Whilst this is unlikely to be useful for replacing \textit{CORSIKA}/\textit{sim\_telarray} for simulating EAS (our attempts at this also failed, due to so called mode-collapse, whereby similar images are repeatedly generated); one potentially useful use of this (proportionately fast) technique would be as a simulator of NSB (using training data similar to that in Chapter \ref{ch:5-CHECNSB}), as GANs appear reasonably reliable at pseudo-simulation of noise (see for example results on CMB data in \cite{darshgan}). 

\subsection{Graph Networks}
Given the results in Chapter \ref{ch:4-VERITASRealData}, it seems unlikely that current CNN-based methods of event classification reliant on charge data (and not utilising timing information as in Chapter \ref{ch:3-TimingInfo}) will in practice be limited by NSB and instrumental noise unless tailcut cleaning is applied.

An increasingly feasible alternative to using new timing based methods would be to use alternative deep learning classifiers to CNNs with charge data. One such method is graph-based Chebyshev networks, where the image data is treated as a 2D connected graph and not a 2D Euclidean array. One advantage of this is that it could potentially skip the image mapping step needed for CNN-type analyses, though recent results from \cite{adithesis} suggest that these methods are currently computationally prohibitive, being too VRAM hungry and slow to be of practical use. Additionally, there is no current means of performing stereoscopic analysis with such techniques. This might change in upcoming years, with research into graph-based methods being currently extremely active, and GPU advances can follow research trends.

\subsection{FPGA Analysis and Photon Stream Analysis}

FACT have recently presented results showing the potential use of Field Programmable Gate Arrays to enhance the speed of deep-learning-based event classification at inference time. Whilst novel and interesting, the fact that this innately limits the analysis to monoscopic reconstruction limits the current potential benefits for sensitivity (since model complexity constraints are enforced when performing such analyses on such simple hardware). However, this may be an interesting avenue of investigation for these analyses methods in the long term. The use of such FPGA-based methods for muon tagging is an interesting potential avenue of investigation. This FACT paper also used timing information in a similar way to our CHEC methods to aid in event classification (albeit in a more computationally costly way than use in Chapter \ref{ch:3-TimingInfo}).

One potential avenue of investigation for SSTCAM is to implement the `photon stream' analysis used by FACT \cite{factphotonstream}; this uses an iterative cross-correlation procedure to extract single photons from their SiPM waveforms, before applying DBSCAN clustering analysis to extract the Cherenkov light. This could lead to improved analysis methods for SSTCAM and might make transfer of such data from Cerro Paranal feasible given the resulting compression factor (this probably wouldn't be the case for full waveforms).

\subsection{Transfer Learning}
One potential solution to the hyperparameter problem for CNN-type methods is the option to take pre-trained and optimised classifiers such as ResNet50 \cite{resnet50} (which is trained on the standard computer science dataset ImageNet) and then slightly retrain the final weights in such a network on IACT data. This is part of the approach used in \cite{tjarkicrc}. However the methods currently used to adapt these methods to stereoscopic analysis are in their infancy, and currently don't appear to offer substantial improvements in sensitivity as a result (possibly because the weights in the network are frozen prior to the sequence analysis stage). Similarly such methods are highly VRAM intensive and as such the overall benefit of using these methods as opposed to training/optimising a classifier from scratch are unclear. That said, there are new deep learning techniques (such as \textit{Perceiver} models \cite{perceiver}) being developed by computer scientists for multi-modal data that may be suitable for IACT analysis that could be used `out the box', albeit currently with somewhat of a drop in performance. Development of such multi-model methods that support ambiguity in the shapes of the input data, will likely be highly useful in future work.

\subsection{Larger Arrays}
Much of the work presented in Chapters \ref{ch:3-TimingInfo} and \ref{ch:4-VERITASRealData} was GPU resource limited, meaning that using simulated arrays truly representative of the SST component of CTA were not possible. Extremely recent, preliminary work by Meiner et al. \cite{tjarkicrc} has shown first CNN-based results from an array of 40 SSTs using the \textit{CTLearn} framework, but this required significant time on the Wilkes-2 GPU cluster at Cambridge to perform. Optimising such networks beyond what has already been performed would be extremely difficult and computationally intensive, but the power of future GPU clusters may make this tractable. This still does not solve the problem with simultaneously utilising multiple telescope classes with Shilon-like deep learning reconstruction methods.

In the long term, even if CNN-type methods can only be demonstrated to offer increased background rejection power for isolated point sources, this could aid in providing sufficient signal-to-noise to perform spectral analysis, which might be necessary for the study of short GRBs from the ground.